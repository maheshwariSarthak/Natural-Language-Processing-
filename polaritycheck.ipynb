{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re \n",
    "import autocorrect\n",
    "import pandas as pd \n",
    "import string \n",
    "from nltk import word_tokenize\n",
    "from string import punctuation\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_nums(text):\n",
    "    cleaned_text = re.sub('<[^<]+?>','', text)\n",
    "    output = ''.join(c for c in cleaned_text if not c.isdigit())\n",
    "    return output\n",
    "#print (remove_html_nums(raw_data))\n",
    "\n",
    "from string import punctuation\n",
    "def remove_punctuation(s):\n",
    "    return ''.join(c for c in s if c not in punctuation)\n",
    "#print (remove_punctuation(raw_data))\n",
    "\n",
    "\"\"\"Converting text to lower case\"\"\"\n",
    "def to_lower(text):\n",
    "    return ' '.join([w.lower() for w in word_tokenize(text)])\n",
    "#print (to_lower(raw_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "#is based on The Porter Stemming Algorithm\n",
    "def stemming(token_data):\n",
    "    snowball_stemmer = SnowballStemmer('english')\n",
    "    stemmed_word = [snowball_stemmer.stem(word) for word in token_data]\n",
    "    return stemmed_word\n",
    "#print(stemming(word_tokenize(tokens)))\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#is based on The Porter Stemming Algorithm\n",
    "def lemmatizing(token_data):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_word = [wordnet_lemmatizer.lemmatize(word) for word in token_data]\n",
    "    return lemmatized_word\n",
    "#print(word_tokenize(tokens))\n",
    "\n",
    "file=open(\"StopWords_Generic.txt\",\"r\")\n",
    "stopwords_eng=file.read()\n",
    "def stop_words(data):\n",
    "    \n",
    "    data_list=[]\n",
    "    #remove stopwords\n",
    "    for word in data:\n",
    "        if word.upper() not in stopwords_eng:\n",
    "            data_list.append(word)\n",
    "    return data_list\n",
    "\n",
    "from autocorrect import Speller\n",
    "spell = Speller(lang='en')\n",
    "def spell_check(token_data):\n",
    "    spells = [spell(w) for w in (token_data)]\n",
    "    return spells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pstv_ngtv(master_dict,data):\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for i in range(len(master_dict)):\n",
    "        s = master_dict.loc[i,\"Word\"] \n",
    "        #print(s)\n",
    "        if s in data :\n",
    "            if master_dict.loc[i,'Positive'] != 0:\n",
    "                pos= pos + 1\n",
    "            if master_dict.loc[i,'Negative'] != 0: \n",
    "                neg= neg + 1\n",
    "    return pos,neg\n",
    "#print(pstv_ngtv(pos_neg,remove))\n",
    "\n",
    "def polarity(x,y):\n",
    "    return (x-y)/(x+y+0.000001)\n",
    "#print(polarity(pos,neg))\n",
    "\n",
    "def subjectivity(pos,neg,data_len):\n",
    "    return (pos+neg)/(data_len+0.000001)\n",
    "\n",
    "def avg_sent_len(word_count,sents_len):\n",
    "    return word_count/sents_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllable_count(word):\n",
    "    word = word.lower()\n",
    "    count = 0\n",
    "    vowels = \"aeiouy\"\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "    if word.endswith(\"e\"):\n",
    "        count -= 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count\n",
    "def complex_cnt(tokens):\n",
    "    cnt =0\n",
    "    for i in tokens :\n",
    "        if syllable_count(i)>1 :\n",
    "            #print(i)\n",
    "            cnt = cnt+1\n",
    "    return cnt;\n",
    "#print(complex_cnt(remove))\n",
    "\n",
    "def percentage_cmplx(cmplx_len,tokens_len):\n",
    "    return (cmplx_len/tokens_len)*100\n",
    "\n",
    "def fog_index(avgsentlen,prcntg_cmplx):\n",
    "    return 0.4*(avgsentlen+prcntg_cmplx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constrain_score(master_dict,data):\n",
    "    cons = 0\n",
    "    for i in range(len(master_dict)):\n",
    "        s = master_dict.loc[i,'Word']\n",
    "        if s in data :\n",
    "            if master_dict.loc[i,'Constraining'] != 0:\n",
    "                cons = cons + 1\n",
    "    return cons\n",
    "\n",
    "def uncertain_score(master_dict,data):\n",
    "    uncer = 0\n",
    "    for i in range(len(master_dict)):\n",
    "        s = master_dict.loc[i,'Word']\n",
    "        if s in data :\n",
    "            if master_dict.loc[i,'Uncertainty'] != 0:\n",
    "                uncer = uncer + 1\n",
    "    return uncer\n",
    "\n",
    "def pos_word_proportion(pos,token_len):\n",
    "    return pos/token_len \n",
    "def neg_word_proportion(neg,token_len):\n",
    "    return neg/token_len\n",
    "def uncertain_word_proportion(uncertain_score,token_len):\n",
    "    return uncertain_score/token_len\n",
    "def constrain_word_proportion(constrain_score,token_len):\n",
    "    return constrain_score/token_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_clean(file_name):\n",
    "    data1 = open(file_name)\n",
    "    raw_data = data1.read() \n",
    "    #print(raw_data)\n",
    "    data =  remove_html_nums(raw_data)\n",
    "    sents_data = sent_tokenize(data)\n",
    "    data1 =  remove_punctuation(data)\n",
    "    data1 = to_lower(data1)\n",
    "    #print(data1)\n",
    "    tokens = word_tokenize(data1)\n",
    "    #print(tokens)\n",
    "    lemm_tokens = lemmatizing(tokens)\n",
    "    #print(lemm_tokens)\n",
    "    clean_tokens = stop_words(lemm_tokens)\n",
    "    #print(clean_tokens)\n",
    "    return clean_tokens,sents_data\n",
    "#print(data_clean(\"blackcoffer1.txt\")[0])\n",
    "#print(data_clean(\"blackcoffer1.txt\")[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dict = pd.read_csv(\"LoughranMcDonald_MasterDictionary_2018.csv\")\n",
    "master_dict[\"Word\"] = master_dict[\"Word\"].str.lower()\n",
    "selected_columns = master_dict[[\"Word\",\"Positive\",\"Negative\",\"Uncertainty\",\"Constraining\"]]\n",
    "dictnry = selected_columns.copy()\n",
    "#dictnry.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comput_variables(data,sents_data,master_dict):\n",
    "    positive_score = pstv_ngtv(master_dict,set(data))[0]\n",
    "    negative_score = pstv_ngtv(master_dict,set(data))[1]\n",
    "    polarity_score = polarity(positive_score,negative_score)\n",
    "    subjectivity_score = subjectivity(positive_score,negative_score,len(data))\n",
    "    avgsentlen = avg_sent_len(len(data),len(sents_data))\n",
    "    complx_count = complex_cnt(set(data))\n",
    "    percent_cmplx = percentage_cmplx(complx_count,len(data))\n",
    "    fog = fog_index(avgsentlen,percent_cmplx)\n",
    "    constrain = constrain_score(master_dict,set(data))\n",
    "    uncertain = uncertain_score(master_dict,set(data))\n",
    "    poswordpropor = pos_word_proportion(positive_score,len(data))\n",
    "    negwordpropor = neg_word_proportion(negative_score,len(data))\n",
    "    uncertwordpropor = uncertain_word_proportion(uncertain,len(data))\n",
    "    constrwordpropor = constrain_word_proportion(constrain,len(data))\n",
    "    return positive_score,negative_score,polarity_score,subjectivity_score,avgsentlen,percent_cmplx,fog,complx_count,len(data),uncertain,constrain,poswordpropor,negwordpropor,uncertwordpropor,constrwordpropor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(data_clean(\"blackcoffer1.txt\")[0])/len(data_clean(\"blackcoffer1.txt\")[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>positive_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>polarity_score</th>\n",
       "      <th>subjectivity_score</th>\n",
       "      <th>average_sentence_length</th>\n",
       "      <th>percentage_of_complex_words</th>\n",
       "      <th>fog_index</th>\n",
       "      <th>complex_word_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>uncertainty_score</th>\n",
       "      <th>constraining_score</th>\n",
       "      <th>positive_word_proportion</th>\n",
       "      <th>negative_word_proportion</th>\n",
       "      <th>uncertainty_word_proportion</th>\n",
       "      <th>constraining_word_proportion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>281.0</td>\n",
       "      <td>-0.556787</td>\n",
       "      <td>0.004094</td>\n",
       "      <td>16.135224</td>\n",
       "      <td>3.894351</td>\n",
       "      <td>8.011830</td>\n",
       "      <td>3434.0</td>\n",
       "      <td>88179.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.000907</td>\n",
       "      <td>0.003187</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>0.000771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>53.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>-0.622776</td>\n",
       "      <td>0.004918</td>\n",
       "      <td>19.046667</td>\n",
       "      <td>4.646482</td>\n",
       "      <td>9.477260</td>\n",
       "      <td>2655.0</td>\n",
       "      <td>57140.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.000928</td>\n",
       "      <td>0.003990</td>\n",
       "      <td>0.000788</td>\n",
       "      <td>0.000875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>26.100000</td>\n",
       "      <td>34.865900</td>\n",
       "      <td>24.386360</td>\n",
       "      <td>182.0</td>\n",
       "      <td>522.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.007663</td>\n",
       "      <td>0.001916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>65.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>-0.543860</td>\n",
       "      <td>0.006394</td>\n",
       "      <td>12.950029</td>\n",
       "      <td>6.663077</td>\n",
       "      <td>7.845242</td>\n",
       "      <td>2970.0</td>\n",
       "      <td>44574.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.001458</td>\n",
       "      <td>0.004936</td>\n",
       "      <td>0.001234</td>\n",
       "      <td>0.001077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>0.012539</td>\n",
       "      <td>25.520000</td>\n",
       "      <td>31.661442</td>\n",
       "      <td>22.872577</td>\n",
       "      <td>202.0</td>\n",
       "      <td>638.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001567</td>\n",
       "      <td>0.010972</td>\n",
       "      <td>0.010972</td>\n",
       "      <td>0.001567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   positive_score  negative_score  polarity_score  subjectivity_score  \\\n",
       "0            80.0           281.0       -0.556787            0.004094   \n",
       "1            53.0           228.0       -0.622776            0.004918   \n",
       "2             0.0             6.0       -1.000000            0.011494   \n",
       "3            65.0           220.0       -0.543860            0.006394   \n",
       "4             1.0             7.0       -0.750000            0.012539   \n",
       "\n",
       "   average_sentence_length  percentage_of_complex_words  fog_index  \\\n",
       "0                16.135224                     3.894351   8.011830   \n",
       "1                19.046667                     4.646482   9.477260   \n",
       "2                26.100000                    34.865900  24.386360   \n",
       "3                12.950029                     6.663077   7.845242   \n",
       "4                25.520000                    31.661442  22.872577   \n",
       "\n",
       "   complex_word_count  word_count  uncertainty_score  constraining_score  \\\n",
       "0              3434.0     88179.0               61.0                68.0   \n",
       "1              2655.0     57140.0               45.0                50.0   \n",
       "2               182.0       522.0                4.0                 1.0   \n",
       "3              2970.0     44574.0               55.0                48.0   \n",
       "4               202.0       638.0                7.0                 1.0   \n",
       "\n",
       "   positive_word_proportion  negative_word_proportion  \\\n",
       "0                  0.000907                  0.003187   \n",
       "1                  0.000928                  0.003990   \n",
       "2                  0.000000                  0.011494   \n",
       "3                  0.001458                  0.004936   \n",
       "4                  0.001567                  0.010972   \n",
       "\n",
       "   uncertainty_word_proportion  constraining_word_proportion  \n",
       "0                     0.000692                      0.000771  \n",
       "1                     0.000788                      0.000875  \n",
       "2                     0.007663                      0.001916  \n",
       "3                     0.001234                      0.001077  \n",
       "4                     0.010972                      0.001567  "
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Output(master_dict) :\n",
    "    df = pd.DataFrame(columns=['positive_score','negative_score','polarity_score','subjectivity_score','average_sentence_length','percentage_of_complex_words','fog_index','complex_word_count','word_count','uncertainty_score','constraining_score','positive_word_proportion','negative_word_proportion','uncertainty_word_proportion','constraining_word_proportion'])\n",
    "\n",
    "    for i in range(152):\n",
    "        s = \"blackcoffer\" + str(i+1) + \".txt\"\n",
    "        arr = comput_variables(data_clean(s)[0],data_clean(s)[1],master_dict)\n",
    "        df2 = {'positive_score' : arr[0],'negative_score' : arr[1],'polarity_score': arr[2],'subjectivity_score' : arr[3],'average_sentence_length' :  arr[4],'percentage_of_complex_words' : arr[5],'fog_index' : arr[6],'complex_word_count' : arr[7],'word_count' : arr[8],'uncertainty_score' : arr[9],'constraining_score' : arr[10],'positive_word_proportion' : arr[11],'negative_word_proportion' : arr[12],'uncertainty_word_proportion' : arr[13],'constraining_word_proportion' : arr[14]}\n",
    "        df = df.append(df2, ignore_index = True)\n",
    "        #df.loc[len(df.index)] = [arr[0],arr[1],arr[2],arr[3],arr[4],arr[5],arr[6],arr[7],arr[8],arr[9],arr[10],arr[11],arr[12],arr[13],arr[14]]\n",
    "    return df\n",
    "a = Output(dictnry)\n",
    "#dictnry.head()\n",
    "#print(data_clean(\"blackcoffer1.txt\")[0])\n",
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.to_csv('result.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
