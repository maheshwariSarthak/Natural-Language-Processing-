{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import langid\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import regex\n",
    "from nltk.classify.textcat import TextCat\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "file_object  = open(\"blackcoffer1.txt\") \n",
    "file = file_object.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarthak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "from html.parser import HTMLParser\n",
    "html_parser = HTMLParser()\n",
    "original_data = html_parser.unescape(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = original_data.encode(\"utf-8\").decode(\"ascii\", errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re    \n",
    "  \n",
    "# remove hyperlinks\n",
    "data = re.sub(r'https?:\\/\\/.\\S+', \"\", data)\n",
    "  \n",
    "# remove hashtags\n",
    "# only removing the hash # sign from the word\n",
    "data = re.sub(r'#', '', data)\n",
    "  \n",
    "# remove old style retweet text \"RT\"\n",
    "data = re.sub(r'^RT[\\s]+', '', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary consisting of the contraction and the actual value\n",
    "Apos_dict={\"'s\":\" is\",\"n't\":\" not\",\"'m\":\" am\",\"'ll\":\" will\",\n",
    "           \"'d\":\" would\",\"'ve\":\" have\",\"'re\":\" are\"}\n",
    "  \n",
    "#replace the contractions\n",
    "for key,value in Apos_dict.items():\n",
    "    if key in data:\n",
    "        data = data.replace(key,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#separate the words\n",
    "data = \" \".join([s for s in re.split(\"([A-Z][a-z]+[^A-Z]*)\",data) if s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to lower case\n",
    "data= data.lower()\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open the fle slang.txt\n",
    "file=open(\"slang.txt\",\"r\")\n",
    "slang=file.read()\n",
    "\n",
    "#seperating each line present in the file\n",
    "slang=slang.split('\\n')\n",
    "\n",
    "data_tokens=data.split()\n",
    "slang_word=[]\n",
    "meaning=[]\n",
    "\n",
    "#store the slang words and meanings in different lists\n",
    "for line in slang:\n",
    "\ttemp=line.split(\"=\")\n",
    "\tslang_word.append(temp[0])\n",
    "\tmeaning.append(temp[-1])\n",
    "\n",
    "#replace the slang word with meaning\n",
    "for i,word in enumerate(data_tokens):\n",
    "\tif word in slang_word:\n",
    "\t\tidx=slang_word.index(word)\n",
    "\t\tdata_tokens[i]=meaning[idx]\n",
    "\t\t\n",
    "data=\" \".join(data_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "#One letter in a word should not be present more than twice in continuation\n",
    "data = ''.join(''.join(s)[:2] for _, s in itertools.groupby(data))\n",
    "\n",
    "\n",
    "from autocorrect import Speller\n",
    "spell = Speller(lang='en')\n",
    "#spell check\n",
    "tweet=spell(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file=open(\"StopWords_Generic.txt\",\"r\")\n",
    "stopwords_eng=file.read()\n",
    "data_tokens=data.split()\n",
    "data_list=[]\n",
    "#remove stopwords\n",
    "for word in data_tokens:\n",
    "\tif word.upper() not in stopwords_eng:\n",
    "\t\tdata_list.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for string operations\n",
    "import string\n",
    "clean_data=[]\n",
    "#remove punctuations\n",
    "for word in data_list:\n",
    "\tif word not in string.punctuation:\n",
    "\t\tclean_data.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = re.sub(r'\\d+', '', clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer() \n",
    "sentence=str(data)\n",
    "sentence = sentence.lower()\n",
    "sentence=sentence.replace('{html}',\"\") \n",
    "cleanr = re.compile('<.*?>')\n",
    "cleantext = re.sub(cleanr, '', sentence)\n",
    "rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(rem_num)  \n",
    "stem_words=[stemmer.stem(w) for w in tokens]\n",
    "lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
